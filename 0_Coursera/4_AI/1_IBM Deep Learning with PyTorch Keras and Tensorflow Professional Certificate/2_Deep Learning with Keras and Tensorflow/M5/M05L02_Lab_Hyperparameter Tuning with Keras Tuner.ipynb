{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Hyperparameter Tuning with Keras Tuner**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to set up Keras Tuner and prepare the environment for hyperparameter tuning. \n",
    "\n",
    "## Learning objectives: \n",
    "By the end of this lab, you will: \n",
    "- Install Keras Tuner and import the necessary libraries\n",
    "- Load and preprocess the MNIST data set\n",
    "- Define a model-building function that uses hyperparameters to configure the model architecture\n",
    "- Set up Keras Tuner to search for the best hyperparameter configuration \n",
    "- Retrieve the best hyperparameters from the search and build a model with these optimized values\n",
    "\n",
    "## Prerequisites: \n",
    "- Basic understanding of Python programming \n",
    "- Keras and TensorFlow installed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Install the Keras Tuner \n",
    "\n",
    "This exercise guides you through the initial setup for using Keras Tuner. You install the library, import necessary modules, and load and preprocess the MNIST data set, which will be used for hyperparameter tuning. \n",
    "1. **Install Keras Tuner:**\n",
    "    - Use pip to install Keras Tuner\n",
    "2. **Import necessary libraries:**\n",
    "    - Import Keras Tuner, TensorFlow, and Keras modules\n",
    "3. **Load and preprocess the MNIST data set:**\n",
    "    - Load the MNIST data set.\n",
    "    - Normalize the data set by dividing by 255.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.16.2\n",
      "  Downloading tensorflow-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (3.15.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (18.1.1)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.2)\n",
      "  Downloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.16.2)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (1.76.0)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.2)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.16.2) (3.13.0)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from tensorflow==2.16.2)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.1.4)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2) (0.1.2)\n",
      "Downloading tensorflow-2.16.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.8/590.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading ml_dtypes-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: protobuf, numpy, tensorboard, ml-dtypes, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.33.2\n",
      "    Uninstalling protobuf-6.33.2:\n",
      "      Successfully uninstalled protobuf-6.33.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.4.0\n",
      "    Uninstalling numpy-2.4.0:\n",
      "      Successfully uninstalled numpy-2.4.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.20.0\n",
      "    Uninstalling tensorboard-2.20.0:\n",
      "      Successfully uninstalled tensorboard-2.20.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.4\n",
      "    Uninstalling ml_dtypes-0.5.4:\n",
      "      Successfully uninstalled ml_dtypes-0.5.4\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.20.0\n",
      "    Uninstalling tensorflow-2.20.0:\n",
      "      Successfully uninstalled tensorflow-2.20.0\n",
      "Successfully installed ml-dtypes-0.3.2 numpy-1.26.4 protobuf-4.25.8 tensorboard-2.16.2 tensorflow-2.16.2\n",
      "Collecting keras-tuner==1.4.7\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.12/site-packages (from keras-tuner==1.4.7) (3.13.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from keras-tuner==1.4.7) (24.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from keras-tuner==1.4.7) (2.32.3)\n",
      "Collecting kt-legacy (from keras-tuner==1.4.7)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (0.1.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (3.15.1)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (0.18.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner==1.4.7) (0.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner==1.4.7) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner==1.4.7) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner==1.4.7) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner==1.4.7) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.12/site-packages (from optree->keras->keras-tuner==1.4.7) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras->keras-tuner==1.4.7) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras->keras-tuner==1.4.7) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner==1.4.7) (0.1.2)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
      "/bin/bash: line 1: 2.0.0: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.16.2\n",
    "!pip install keras-tuner==1.4.7\n",
    "!pip install numpy<2.0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: \n",
    "This code installs the necessary libraries using pip\n",
    "\n",
    "- **TensorFlow**: Ensures compatibility with the Keras Tuner.\n",
    "- **Keras Tuner**: The version used in this lab.\n",
    "- **Numpy**: Ensures compatibility with the other installed packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Increase recursion limit to prevent potential issues\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation: \n",
    "The sys.setrecursionlimit function is used to increase the recursion limit, which helps prevent potential recursion errors when running complex models with deep nested functions or when using certain libraries like TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-21 03:11:41.432978: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-21 03:11:41.434226: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-21 03:11:41.439829: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-21 03:11:41.454150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-21 03:11:41.480812: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-21 03:11:41.480901: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-21 03:11:41.499451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-21 03:11:42.873322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import necessary libraries\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress all Python warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set TensorFlow log level to suppress warnings and info messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all logs, 1 = filter out INFO, 2 = filter out INFO and WARNING, 3 = ERROR only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m176.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m207.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_all, y_all), _ = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Flatten and normalize the images\n",
    "x_all = x_all.reshape((x_all.shape[0], -1)).astype(\"float32\") / 255.0\n",
    "\n",
    "# Split into train+val and test (80/20)\n",
    "x_temp, x_test, y_temp, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split train+val into train and validation (75/25 of 80% = 60/20 overall)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This code imports the necessary libraries:\n",
    "\n",
    "- **`keras_tuner`**: Used for hyperparameter tuning.\n",
    "- **`Sequential`**: A linear stack of layers in Keras.\n",
    "- **`Dense`**, **`Flatten`**: Common Keras layers.\n",
    "- **`mnist`**: The MNIST dataset, a standard dataset for image classification.\n",
    "- **`Adam`**: An optimizer in Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Validation data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "\n",
    "print(f'Training data shape: {x_train.shape}')\n",
    "print(f'Validation data shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This code loads the MNIST dataset and preprocesses it:\n",
    "\n",
    "- **`mnist.load_data()`**: Loads the dataset, returning training and validation splits.\n",
    "- **`x_train / 255.0`**: Normalizes the pixel values to be between 0 and 1.\n",
    "- **`print(f'...')`**: Displays the shapes of the training and validation datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Defining the model with hyperparameters \n",
    "\n",
    "In this exercise, you define a model-building function that uses the `HyperParameters` object to specify the number of units in a dense layer and the learning rate. This function returns a compiled Keras model that is ready for hyperparameter tuning.\n",
    "\n",
    "**Define a model-building function:**\n",
    "- Create a function `build_model` that takes a `HyperParameters` object as input.\n",
    "- Use the `HyperParameters` object to define the number of units in a dense layer and the learning rate for the optimizer.\n",
    "- Compile the model with sparse categorical cross-entropy loss and Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model-building function \n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This function builds and compiles a Keras model with hyperparameters:\n",
    "\n",
    "- **`hp.Int('units', ...)`**: Defines the number of units in the Dense layer as a hyperparameter.\n",
    "- **`hp.Float('learning_rate', ...)`**: Defines the learning rate as a hyperparameter.\n",
    "- **`model.compile()`**: Compiles the model with the Adam optimizer and sparse categorical cross-entropy loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Configuring the hyperparameter search \n",
    "\n",
    "This exercise guides you through configuring Keras Tuner. You create a `RandomSearch` tuner, specifying the model-building function, the optimization objective, the number of trials, and the directory for storing results. The search space summary provides an overview of the hyperparameters being tuned. \n",
    "\n",
    "**Create a RandomSearch Tuner:**\n",
    "- Use the `RandomSearch` class from Keras Tuner. \n",
    "- Specify the model-building function, optimization objective (validation accuracy), number of trials, and directory for storing results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "# Create a RandomSearch Tuner \n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "# Display a summary of the search space \n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This code sets up a Keras Tuner `RandomSearch`:\n",
    "\n",
    "- **`build_model`**: The model-building function.\n",
    "- **`objective='val_accuracy'`**: The metric to optimize (validation accuracy).\n",
    "- **`max_trials=10`**: The maximum number of different hyperparameter configurations to try.\n",
    "- **`executions_per_trial=2`**: The number of times to run each configuration.\n",
    "- **`directory='my_dir'`**: Directory to save the results.\n",
    "- **`project_name='intro_to_kt'`**: Name of the project for organizing results.\n",
    "\n",
    "Displays a summary of the hyperparameter search space, providing an overview of the hyperparameters being tuned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Running the hyperparameter search \n",
    "\n",
    "In this exercise, you run the hyperparameter search using the `search` method of the tuner. You provide the training and validation data along with the number of epochs. After the search is complete, the results summary displays the best hyperparameter configurations found. \n",
    "\n",
    "**Run the search:**\n",
    "- Use the `search` method of the tuner. \n",
    "- Pass in the training data, validation data, and the number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 04m 10s]\n",
      "val_accuracy: 0.98089998960495\n",
      "\n",
      "Best val_accuracy So Far: 0.98089998960495\n",
      "Total elapsed time: 00h 35m 13s\n",
      "Results summary\n",
      "Results in my_dir/intro_to_kt\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.0005566443210859141\n",
      "Score: 0.98089998960495\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.0009038179826460621\n",
      "Score: 0.9805500209331512\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.0012250643949698182\n",
      "Score: 0.979500025510788\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.00186820564517529\n",
      "Score: 0.9787499904632568\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 160\n",
      "learning_rate: 0.0018831577513121129\n",
      "Score: 0.9772500097751617\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.0031348941018167933\n",
      "Score: 0.9757499992847443\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.0002323209012380704\n",
      "Score: 0.9732999801635742\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.005737668868809639\n",
      "Score: 0.9720500111579895\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 0.007775645935042448\n",
      "Score: 0.9662500023841858\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.0005317095449862572\n",
      "Score: 0.9580000042915344\n"
     ]
    }
   ],
   "source": [
    "# Run the hyperparameter search \n",
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val)) \n",
    "\n",
    "# Display a summary of the results \n",
    "tuner.results_summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This command runs the hyperparameter search:\n",
    "\n",
    "- **`epochs=5`**: Each trial is trained for 5 epochs.\n",
    "- **`validation_data=(x_val, y_val)`**: The validation data to evaluate the model's performance during the search.\n",
    "\n",
    "After the search is complete, this command displays a summary of the best hyperparameter configurations found during the search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Analyzing and using the best hyperparameters \n",
    "\n",
    "In this exercise, you retrieve the best hyperparameters found during the search and print their values. You then build a model with these optimized hyperparameters and train it on the full training data set. Finally, you evaluate the model’s performance on the test set to ensure that it performs well with the selected hyperparameters. \n",
    "\n",
    "**Retrieve the best hyperparameters:**\n",
    "- Use the `get_best_hyperparameters` method to get the best hyperparameters. \n",
    "- Print the optimal values for the hyperparameters. \n",
    "\n",
    "**Build and train the model:**\n",
    "- Build a model using the best hyperparameters. \n",
    "- Train the model on the full training data set and evaluate its performance on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "The optimal number of units in the first dense layer is 320. \n",
      "\n",
      "The optimal learning rate for the optimizer is 0.0005566443210859141. \n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9196 - loss: 0.2871 - val_accuracy: 0.9601 - val_loss: 0.1483\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9643 - loss: 0.1227 - val_accuracy: 0.9666 - val_loss: 0.1148\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9766 - loss: 0.0809 - val_accuracy: 0.9714 - val_loss: 0.0991\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9833 - loss: 0.0586 - val_accuracy: 0.9746 - val_loss: 0.0829\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9876 - loss: 0.0427 - val_accuracy: 0.9771 - val_loss: 0.0768\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - accuracy: 0.9906 - loss: 0.0322 - val_accuracy: 0.9770 - val_loss: 0.0769\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9935 - loss: 0.0246 - val_accuracy: 0.9762 - val_loss: 0.0784\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9949 - loss: 0.0185 - val_accuracy: 0.9776 - val_loss: 0.0791\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9959 - loss: 0.0148 - val_accuracy: 0.9769 - val_loss: 0.0816\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9971 - loss: 0.0114 - val_accuracy: 0.9789 - val_loss: 0.0823\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9812 - loss: 0.0628   \n",
      "Test accuracy: 0.9811999797821045\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Retrieve the best hyperparameters \n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "print(f\"\"\" \n",
    "\n",
    "The optimal number of units in the first dense layer is {best_hps.get('units')}. \n",
    "\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}. \n",
    "\n",
    "\"\"\") \n",
    "\n",
    "# Step 2: Build and Train the Model with Best Hyperparameters \n",
    "model = tuner.hypermodel.build(best_hps) \n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2) \n",
    "\n",
    "# Evaluate the model on the test set \n",
    "test_loss, test_acc = model.evaluate(x_val, y_val) \n",
    "print(f'Test accuracy: {test_acc}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "This code retrieves the best hyperparameters found during the search:\n",
    "\n",
    "- **`get_best_hyperparameters(num_trials=1)`**: Gets the best hyperparameter configuration.\n",
    "- **`print(f\"...\")`**: Prints the best hyperparameters.\n",
    "- **`model.fit(...)`**: Trains the model on the full training data with a validation split of 20%.\n",
    "- **`model.evaluate(...)`**: Evaluates the model on the test (validation) dataset and prints the accuracy, which gives an indication of how well the model generalizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice exercises \n",
    "\n",
    "### Exercise 1: Setting Up Keras Tuner \n",
    "\n",
    "#### Objective: \n",
    "Learn how to set up Keras Tuner and prepare the environment for hyperparameter tuning. \n",
    "\n",
    "#### Instructions: \n",
    "1. Install Keras Tuner.\n",
    "2. Import necessary libraries.\n",
    "3. Load and preprocess the MNIST data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in /opt/conda/lib/python3.12/site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.12/site-packages (from keras-tuner) (3.13.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from keras-tuner) (24.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.12/site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner) (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner) (14.2.0)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner) (0.1.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner) (3.15.1)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner) (0.18.0)\n",
      "Requirement already satisfied: ml-dtypes in /opt/conda/lib/python3.12/site-packages (from keras->keras-tuner) (0.3.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->keras-tuner) (2024.12.14)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.12/site-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras->keras-tuner) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras->keras-tuner) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
      "Training data shape: (60000, 28, 28)\n",
      "Validation data shape: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "!pip install keras-tuner \n",
    "\n",
    "# Step 2: Import necessary libraries \n",
    "import keras_tuner as kt \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "# Step 3: Load and preprocess the MNIST data set \n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data() \n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0 \n",
    "\n",
    "# Print the shapes of the training and validation datasets\n",
    "print(f'Training data shape: {x_train.shape}') \n",
    "print(f'Validation data shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "!pip install keras-tuner \n",
    "\n",
    "# Step 2: Import necessary libraries \n",
    "import keras_tuner as kt \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.datasets import mnist \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "# Step 3: Load and preprocess the MNIST data set \n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data() \n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0 \n",
    "\n",
    "# Print the shapes of the training and validation datasets\n",
    "print(f'Training data shape: {x_train.shape}') \n",
    "print(f'Validation data shape: {x_val.shape}')\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Defining the model with hyperparameters \n",
    "\n",
    "#### Objective: \n",
    "Define a model-building function that uses hyperparameters to configure the model architecture. \n",
    "\n",
    "#### Instructions: \n",
    "1. Define a model-building function that uses the `HyperParameters` object to specify the number of units in a dense layer and the learning rate. \n",
    "2. Compile the model with sparse categorical cross-entropy loss and Adam optimizer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Step 1: Define a model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Step 1: Define a model-building function\n",
    "def build_model(hp):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Configuring the hyperparameter search \n",
    "\n",
    "#### Objective: \n",
    "Set up Keras Tuner to search for the best hyperparameter configuration. \n",
    "\n",
    "#### Instructions: \n",
    "1. Create a `RandomSearch` tuner using the model-building function. \n",
    "2. Specify the optimization objective, number of trials, and directory for storing results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir/intro_to_kt/tuner0.json\n",
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': 'linear'}\n",
      "learning_rate (Float)\n",
      "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Step 1: Create a RandomSearch Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,  # Ensure 'build_model' function is defined from previous code\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "# Display a summary of the search space\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Step 1: Create a RandomSearch Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,  # Ensure 'build_model' function is defined from previous code\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='intro_to_kt'\n",
    ")\n",
    "\n",
    "# Display a summary of the search space\n",
    "tuner.search_space_summary()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Running the hyperparameter search\n",
    "\n",
    "#### Objective: \n",
    "Run the hyperparameter search and dispaly the summary of the results.\n",
    "\n",
    "#### Instructions: \n",
    "1. Run the hyperparameter search using the `search` method of the tuner. \n",
    "2. Pass in the training data, validation data, and the number of epochs. \n",
    "3. Display a summary of the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in my_dir/intro_to_kt\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.0005566443210859141\n",
      "Score: 0.98089998960495\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.0009038179826460621\n",
      "Score: 0.9805500209331512\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.0012250643949698182\n",
      "Score: 0.979500025510788\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.00186820564517529\n",
      "Score: 0.9787499904632568\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 160\n",
      "learning_rate: 0.0018831577513121129\n",
      "Score: 0.9772500097751617\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 352\n",
      "learning_rate: 0.0031348941018167933\n",
      "Score: 0.9757499992847443\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 256\n",
      "learning_rate: 0.0002323209012380704\n",
      "Score: 0.9732999801635742\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 384\n",
      "learning_rate: 0.005737668868809639\n",
      "Score: 0.9720500111579895\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 0.007775645935042448\n",
      "Score: 0.9662500023841858\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 0.0005317095449862572\n",
      "Score: 0.9580000042915344\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# Step 1: Run the hyperparameter search \n",
    "\n",
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val)) \n",
    "\n",
    " # Display a summary of the results \n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Run the hyperparameter search \n",
    "\n",
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val)) \n",
    "\n",
    " # Display a summary of the results \n",
    "\n",
    "tuner.results_summary()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Analyzing and using the best hyperparameters \n",
    "\n",
    "#### Objective: \n",
    "Retrieve the best hyperparameters from the search and build a model with these optimized values. \n",
    "\n",
    "#### Instructions: \n",
    "1. Retrieve the best hyperparameters using the `get_best_hyperparameters` method. \n",
    "2. Build a model using the best hyperparameters. \n",
    "3. Train the model on the full training data set and evaluate its performance on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "The optimal number of units in the first dense layer is 320. \n",
      "\n",
      "The optimal learning rate for the optimizer is 0.0005566443210859141. \n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - accuracy: 0.9182 - loss: 0.2919 - val_accuracy: 0.9566 - val_loss: 0.1535\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9640 - loss: 0.1253 - val_accuracy: 0.9635 - val_loss: 0.1236\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9759 - loss: 0.0823 - val_accuracy: 0.9714 - val_loss: 0.0910\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9829 - loss: 0.0596 - val_accuracy: 0.9727 - val_loss: 0.0916\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9873 - loss: 0.0437 - val_accuracy: 0.9734 - val_loss: 0.0855\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9909 - loss: 0.0323 - val_accuracy: 0.9749 - val_loss: 0.0844\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9925 - loss: 0.0258 - val_accuracy: 0.9753 - val_loss: 0.0766\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.9947 - loss: 0.0197 - val_accuracy: 0.9772 - val_loss: 0.0799\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9964 - loss: 0.0143 - val_accuracy: 0.9753 - val_loss: 0.0837\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.9973 - loss: 0.0114 - val_accuracy: 0.9770 - val_loss: 0.0810\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9776 - loss: 0.0780   \n",
      "Validation accuracy: 0.9775999784469604\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# Step 1: Retrieve the best hyperparameters \n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "\n",
    "print(f\"\"\" \n",
    "\n",
    "The optimal number of units in the first dense layer is {best_hps.get('units')}. \n",
    "\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}. \n",
    "\n",
    "\"\"\") \n",
    "\n",
    " # Step 2: Build and train the model with best hyperparameters \n",
    "\n",
    "model = tuner.hypermodel.build(best_hps) \n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2) \n",
    "\n",
    " # Evaluate the model on the validation set \n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_val, y_val) \n",
    "\n",
    "print(f'Validation accuracy: {val_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "# Step 1: Retrieve the best hyperparameters \n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0] \n",
    "\n",
    "print(f\"\"\" \n",
    "\n",
    "The optimal number of units in the first dense layer is {best_hps.get('units')}. \n",
    "\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}. \n",
    "\n",
    "\"\"\") \n",
    "\n",
    " # Step 2: Build and train the model with best hyperparameters \n",
    "\n",
    "model = tuner.hypermodel.build(best_hps) \n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2) \n",
    "\n",
    " # Evaluate the model on the validation set \n",
    "\n",
    "val_loss, val_acc = model.evaluate(x_val, y_val) \n",
    "\n",
    "print(f'Validation accuracy: {val_acc}') \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "Congratulations on completing this lab! You have learned to set up Keras Tuner and prepare the environment for hyperparameter tuning. In addition, you defined a model-building function that uses hyperparameters to configure the model architecture. You configured Keras Tuner to search for the best hyperparameter configuration and learned to run the hyperparameter search and analyze the results. Finally, you retrieved the best hyperparameters and built a model with these optimized values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skillup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright ©IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "4480f294d0e5af91350ef1c70e7b3bd8f76b50e8822bcb90342d59ff1810e228"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
