{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b546d597-3d20-4fb2-9dbb-b62c54b9344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets  python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99832810-f8ca-43c3-8ffd-5ad61f7ba356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers[torch] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "983a2af3-912d-4660-8981-91dcbb4105db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Specify the path to env.txt\n",
    "load_dotenv(\"env.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b1dd090-d613-4c8b-9a5f-2b9d1a29aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab64611-1a0d-49bb-bd06-20924a68ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9c7ab1-d334-4401-8181-3f93042317ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a8ba8aebdf45bc881efcf0e030e34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a tiny dataset subset\n",
    "dataset = load_dataset(\"squad\", split=\"train[:100]\")  # Only 100 examples\n",
    "eval_dataset = load_dataset(\"squad\", split=\"validation[:20]\")  # 20 validation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8666d75-86e7-4cbe-a469-6b5ec9d0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load smaller model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # Much smaller than BERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723fce0f-aff5-49ad-a6cc-8e116600efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize questions and contexts\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],  # Now passing lists directly\n",
    "        examples[\"context\"],   # Now passing lists directly\n",
    "        truncation=\"only_second\",\n",
    "        max_length=256,\n",
    "        stride=128,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=False  # Disable overflow tokens for simplicity\n",
    "    )\n",
    "    \n",
    "    # Initialize answer arrays\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # Process each example in the batch\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        # Get the offset mapping for this example\n",
    "        offset = tokenized[\"offset_mapping\"][i]\n",
    "        \n",
    "        # Get answer for this example\n",
    "        answer = examples[\"answers\"][i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        \n",
    "        # Convert char positions to token positions\n",
    "        start_token = 0\n",
    "        end_token = 0\n",
    "        \n",
    "        # Find the token positions that contain the answer\n",
    "        for idx, (start, end) in enumerate(offset):\n",
    "            if start <= start_char and end >= start_char:\n",
    "                start_token = idx\n",
    "            if start <= end_char and end >= end_char:\n",
    "                end_token = idx\n",
    "                break\n",
    "        \n",
    "        start_positions.append(start_token)\n",
    "        end_positions.append(end_token)\n",
    "    \n",
    "    # Add answer positions to tokenized output\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdc2a13-c054-4337-b702-0feafa829379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process datasets\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Explicit batch size\n",
    "    remove_columns=dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac29d86-daf4-4149-9c13-b2ccee9a8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32,  # Explicit batch size\n",
    "    remove_columns=eval_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f4297f-4fb1-4654-92fc-ca2b648165c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./quick-qa-results\",\n",
    "    num_train_epochs=1,  # Single epoch\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,  # Slightly higher learning rate\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"no\",  # Skip evaluation to save time\n",
    "    save_strategy=\"no\",  # Don't save checkpoints\n",
    "    use_cpu=True,  # Force CPU\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard reporting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41781111-b05c-4606-896b-ce1dfbd6a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f06eeb-7ace-4425-aded-34be17bbb680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.210500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./quick-qa-model/tokenizer_config.json',\n",
       " './quick-qa-model/special_tokens_map.json',\n",
       " './quick-qa-model/vocab.txt',\n",
       " './quick-qa-model/added_tokens.json',\n",
       " './quick-qa-model/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and save\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./quick-qa-model\")\n",
    "tokenizer.save_pretrained(\"./quick-qa-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e0762b9-166e-4d69-82d1-5a780a57aec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qa_model(model_path=\"./quick-qa-model\"):\n",
    "    # Load model and tokenizer from saved directory\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ba560d-00c8-4424-a230-d037fe6555e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_question(question, context, model, tokenizer):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=256,\n",
    "        truncation=\"only_second\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Find start and end positions\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    # Convert token positions to string\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    answer = tokenizer.convert_tokens_to_string(tokens[answer_start:answer_end + 1])\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a02a8fcd-0dac-4f25-b2ba-bddfb7fac5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Who created Python?\n",
      "Answer: 1991\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_qa_model()\n",
    "\n",
    "# Example context and question\n",
    "context = \"\"\"\n",
    "Python is a high-level programming language created by Guido van Rossum and released in 1991. \n",
    "Python's design emphasizes code readability with its notable use of significant whitespace. \n",
    "Its language constructs and object-oriented approach aim to help programmers write clear, logical code.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who created Python?\"\n",
    "\n",
    "# Get answer\n",
    "answer = answer_question(question, context, model, tokenizer)\n",
    "print(f\"\\nQuestion: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435064c8-3c47-4835-b21c-d3f1afedfea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
